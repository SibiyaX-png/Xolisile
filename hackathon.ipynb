{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ec0ed2",
   "metadata": {
    "id": "81ec0ed2"
   },
   "source": [
    "<a id=\"top\"></a>\n",
    "#  **Classification Hackathon** \n",
    "## Xolisile Sibiya <sup> </sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9565abd",
   "metadata": {
    "id": "d9565abd"
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd672a8e",
   "metadata": {
    "id": "fd672a8e"
   },
   "source": [
    "South Africa is a multicultural society that is characterised by its rich linguistic diversity. Language is an indispensable tool that can be used to deepen democracy and also contribute to the social, cultural, intellectual, economic and political life of the South African society.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The country is multilingual with 11 official languages, each of which is guaranteed equal status. Most South Africans are multilingual and able to speak at least two or more of the official languages.With such a multilingual population, it is only obvious that our systems and devices also communicate in multi-languages.\n",
    "\n",
    "### Objectives\n",
    "The key objective is to build a Machine Learning model that will take text which is in any of South Africa's 11 Official languages and identify which language the text is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edc932a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0edc932a",
    "outputId": "f3adb9b2-7c76-4fbf-9c37-8cb83771520b"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "# For searching patterns on the tweets (regex)\n",
    "import re\n",
    "# datetime\n",
    "import datetime\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "from sklearn.pipeline import Pipeline\n",
    "import statsmodels.formula.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.stats import boxcox, zscore\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "\n",
    "# visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "# saving my model\n",
    "import pickle\n",
    "\n",
    "#ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa9a342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0fa9a342",
    "outputId": "67e1f45c-a822-4d5e-cf76-1631728a0136"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xho</td>\n",
       "      <td>umgaqo-siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xho</td>\n",
       "      <td>i-dha iya kuba nobulumko bokubeka umsebenzi na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>the province of kwazulu-natal department of tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nso</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ven</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang_id                                               text\n",
       "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...\n",
       "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...\n",
       "2     eng  the province of kwazulu-natal department of tr...\n",
       "3     nso  o netefatša gore o ba file dilo ka moka tše le...\n",
       "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "df_test = pd.read_csv('test_set.csv')\n",
    "df_train = pd.read_csv('train_set.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa886db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5fa886db",
    "outputId": "4d74151b-ac2e-4745-a282-3240f6809bb5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mmasepala, fa maemo a a kgethegileng a letlele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Uzakwaziswa ngokufaneleko nakungafuneka eminye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tshivhumbeo tshi fana na ngano dza vhathu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Kube inja nelikati betingevakala kutsi titsini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Winste op buitelandse valuta.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text\n",
       "0      1  Mmasepala, fa maemo a a kgethegileng a letlele...\n",
       "1      2  Uzakwaziswa ngokufaneleko nakungafuneka eminye...\n",
       "2      3         Tshivhumbeo tshi fana na ngano dza vhathu.\n",
       "3      4  Kube inja nelikati betingevakala kutsi titsini...\n",
       "4      5                      Winste op buitelandse valuta."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f119f4",
   "metadata": {
    "id": "e9f119f4"
   },
   "source": [
    "### Data Overview\n",
    "\n",
    "**Train dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d394c7c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d394c7c8",
    "outputId": "0a37fc56-4108-4456-a19d-5435962ebc30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows    :  33000\n",
      "Columns :  2\n",
      "\n",
      "Missing values:  0\n",
      "\n",
      "Information about the data: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33000 entries, 0 to 32999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   lang_id  33000 non-null  object\n",
      " 1   text     33000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 515.8+ KB\n",
      "  \n",
      " None\n",
      "\n",
      "About the data: \n",
      "\n",
      "Feature 'lang_id' has 11 unique categories\n",
      "Feature 'text' has 29948 unique categories\n"
     ]
    }
   ],
   "source": [
    "# Checking how our training dataset looks like\n",
    "print(\"Rows    : \", df_train.shape[0])\n",
    "\n",
    "print(\"Columns : \", df_train.shape[1])\n",
    "\n",
    "print(\"\\nMissing values: \", df_train.isnull().sum().values.sum())\n",
    "\n",
    "print(\"\\nInformation about the data: \")\n",
    "print(\"  \\n\", df_train.info())\n",
    " \n",
    "print(\"\\nAbout the data: \\n\")\n",
    "\n",
    "# Check how many unique items are in each column of the dateframe\n",
    "for col_name in df_train.columns:\n",
    "    unique_out = len(df_train[col_name].unique())\n",
    "    print(f\"Feature '{col_name}' has {unique_out} unique categories\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36746705",
   "metadata": {
    "id": "36746705"
   },
   "source": [
    "**Test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc875a75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc875a75",
    "outputId": "8cae8767-2e3e-46e4-e664-4e9baa68220b"
   },
   "outputs": [],
   "source": [
    "# Checking how our data looks like\n",
    "print(\"Rows    : \", df_test.shape[0])\n",
    "\n",
    "print(\"Columns : \", df_test.shape[1])\n",
    "\n",
    "print(\"\\nMissing values: \", df_test.isnull().sum().values.sum())\n",
    "\n",
    "print(\"\\nInformation about the data: \")\n",
    "print(\"  \\n\", df_test.info())\n",
    " \n",
    "print(\"\\nAbout the data: \\n\")\n",
    "\n",
    "# Check how many unique items are in each column of the dateframe\n",
    "for col_name in df_test.columns:\n",
    "    unique_out = len(df_test[col_name].unique())\n",
    "    print(f\"Feature '{col_name}' has {unique_out} unique categories\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UN3HlY72BmG3",
   "metadata": {
    "id": "UN3HlY72BmG3"
   },
   "source": [
    "<a id=\"cleaning\"></a>\n",
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63eb017",
   "metadata": {
    "id": "c63eb017"
   },
   "source": [
    "Data preprocessing is a technique that involves taking in raw data and transforming it into a understable format. The technique includes data cleaning, intergration, transformation, reduction and discretization. The data preprocessing plan will include the following processes:\n",
    "\n",
    "- **Data cleaning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78450afd",
   "metadata": {
    "id": "78450afd"
   },
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc0753",
   "metadata": {
    "id": "ebbc0753"
   },
   "source": [
    "Data cleaning is a process of improving the quality of the data by identifying corrupt or erroneous records from a data set and rectifying them.\n",
    "\n",
    "The data cleaning process will include the following:\n",
    "- Expanding contractions where necessary\n",
    "- Removal of the noise:\n",
    "    - punctuations\n",
    "    - numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5JmjrWTC3zxe",
   "metadata": {
    "id": "5JmjrWTC3zxe"
   },
   "source": [
    "#### Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88faa14d",
   "metadata": {
    "id": "88faa14d"
   },
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    text = text.lower() # making text to be lowercase\n",
    "    return text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lowercase)\n",
    "\n",
    "df_test['text'] = df_test['text'].apply(lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc81be",
   "metadata": {
    "id": "f9bc81be"
   },
   "source": [
    "#### Remove Noise from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DeMiKw4_9-n9",
   "metadata": {
    "id": "DeMiKw4_9-n9"
   },
   "source": [
    "Data that can not be processed/interpreted by a machine is classified as noisy data. Text data contains a lot of noise, this comes in a  form of special characters punctuation and numbers. During this process the data will be changed from accent letters to normal letters and the noise will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Y2nzYYT_80S5",
   "metadata": {
    "id": "Y2nzYYT_80S5"
   },
   "outputs": [],
   "source": [
    " \n",
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\"takes string of text and return a string of text with no punctuations,\n",
    "    white spaces \"\"\"\n",
    "    \n",
    "    text = re.sub(\"\\\\s+\", \" \", text)  # Remove extra whitespace\n",
    "                  \n",
    "    text = re.sub(\"(\\#)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\-)\", \" \", text) # replace punctuation with space\n",
    "    \n",
    "    text = re.sub(\"(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\",\" \",text)# replace punctuation with space\n",
    "    \n",
    "    text = text.lstrip()  # removes whitespaces before string\n",
    "    \n",
    "    text = text.rstrip()  # removes whitespaces after string\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_train['clean_text'] = df_train['text'].apply(clean_text)\n",
    "\n",
    "df_test['clean_text'] = df_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a709973",
   "metadata": {
    "id": "3a709973"
   },
   "outputs": [],
   "source": [
    "# remove numbers\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \n",
    "    \"\"\"takes a string of text and remove numbers\"\"\"\n",
    "    \n",
    "    number_pattern = r'\\d+'\n",
    "    \n",
    "    without_number = re.sub(pattern = number_pattern, repl = \" \", string = text)\n",
    "    \n",
    "    return without_number\n",
    "\n",
    "df_train['clean_text'] = df_train['clean_text'].apply(lambda x: remove_numbers(x))\n",
    "\n",
    "df_test['clean_text'] = df_test['clean_text'].apply(lambda x: remove_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12eea049",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12eea049",
    "outputId": "200c1537-5a78-400d-df78-f0b4f2f5a6fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xho</td>\n",
       "      <td>umgaqo-siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "      <td>umgaqo siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xho</td>\n",
       "      <td>i-dha iya kuba nobulumko bokubeka umsebenzi na...</td>\n",
       "      <td>i dha iya kuba nobulumko bokubeka umsebenzi na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>the province of kwazulu-natal department of tr...</td>\n",
       "      <td>the province of kwazulu natal department of tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nso</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ven</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang_id                                               text  \\\n",
       "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...   \n",
       "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...   \n",
       "2     eng  the province of kwazulu-natal department of tr...   \n",
       "3     nso  o netefatša gore o ba file dilo ka moka tše le...   \n",
       "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  umgaqo siseko wenza amalungiselelo kumaziko ax...  \n",
       "1  i dha iya kuba nobulumko bokubeka umsebenzi na...  \n",
       "2  the province of kwazulu natal department of tr...  \n",
       "3  o netefatša gore o ba file dilo ka moka tše le...  \n",
       "4  khomishini ya ndinganyiso ya mbeu yo ewa maana...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe277226",
   "metadata": {
    "id": "fe277226"
   },
   "source": [
    "<a id=\"features\"></a>\n",
    "## **Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LEqWtz5qG4Ac",
   "metadata": {
    "id": "LEqWtz5qG4Ac"
   },
   "source": [
    "The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process.\n",
    "\n",
    "The training data must contain the correct answer, which is known as a target or target attribute. The learning algorithm finds patterns in the training data that map the input data attributes to the target (the answer that you want to predict), and it outputs an ML model that captures these patterns.\n",
    "We train different models on the training data.\n",
    "\n",
    "- **The training** set is a subset of the dataset to build predictive models.\n",
    "\n",
    "- **Test** set or unseen examples is a subset of the dataset to assess the likely future performance of a model. If a model fit to the training set much better than it fits the test set, overfitting is probably the cause."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50706fb7",
   "metadata": {
    "id": "50706fb7"
   },
   "source": [
    "Data is randomly split into training and validation data sets. 80% is for training the model and 20% is for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a3de57",
   "metadata": {
    "id": "65a3de57"
   },
   "outputs": [],
   "source": [
    "y = df_train['lang_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ZgvD8L3_4FRh",
   "metadata": {
    "id": "ZgvD8L3_4FRh"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train['clean_text'],\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a9db0",
   "metadata": {
    "id": "1a4a9db0"
   },
   "source": [
    "Models are put in a pipeline with a vectorizer. Machine learning algorithms operate on a numeric feature space, expecting input as a two-dimensional array where rows are instances and columns are features. In order to perform machine learning on text, we need to transform our documents into vector representations such that we can apply numeric machine learning. This process is called feature extraction or more simply, vectorization, and is an essential first step toward language-aware analysis.\n",
    "\n",
    "- CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample.\n",
    "\n",
    "- Frequency–Inverse Document Frequency Vectorizer Transforms text to feature vectors that can be used as input to estimator. vocabulary_ Is a dictionary that converts each token (word) to feature index in the matrix, each unique token gets a feature index. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cea61e",
   "metadata": {
    "id": "c2cea61e"
   },
   "source": [
    "**Linear Support Vector Classification**\n",
    "\n",
    "A support vector machine takes these data points and outputs the hyperplane (which in two dimensions it's simply a line) that best separates the tags. This line is the decision boundary.\n",
    "Linear SVC (Support Vector Classifier) fits to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data.\n",
    "\n",
    "Similar to SVC with parameter kernel = ’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac8941",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51ac8941",
    "outputId": "38b3c25c-13e0-47f4-8495-98c5e17a0075"
   },
   "outputs": [],
   "source": [
    "# Linear Support Vector Classifier with TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MaxAbsScaler()\n",
    "\n",
    "lsvc = Pipeline([('tfidf', TfidfVectorizer(ngram_range = (1,3))), \n",
    "                 \n",
    "                 ('scaler', preprocessing.MaxAbsScaler()),\n",
    "                 \n",
    "                 ('lsvc', LinearSVC(C = 5, class_weight ='balanced',\n",
    "                                  max_iter = 8000))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65992c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc.fit(df_train['clean_text'], df_train['lang_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91646c6d",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fmCCtCm0XxGB",
   "metadata": {
    "id": "fmCCtCm0XxGB"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "#pred2 = snb_classifier.predict(X_test_scaled)\n",
    "nb = Pipeline([('tfidf', TfidfVectorizer(stop_words = 'english',\n",
    "                                         ngram_range = (1,2))), \n",
    "                 \n",
    "                 ('scaler', preprocessing.MaxAbsScaler()),\n",
    "                 \n",
    "                 ('nb',MultinomialNB())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a0432f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 2), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;scaler&#x27;, MaxAbsScaler()), (&#x27;nb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 2), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;scaler&#x27;, MaxAbsScaler()), (&#x27;nb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 2), stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MaxAbsScaler</label><div class=\"sk-toggleable__content\"><pre>MaxAbsScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(ngram_range=(1, 2), stop_words='english')),\n",
       "                ('scaler', MaxAbsScaler()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365965f9",
   "metadata": {
    "id": "365965f9"
   },
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dSIm-LfpTtg2",
   "metadata": {
    "id": "dSIm-LfpTtg2"
   },
   "source": [
    "Model Evaluation is an integral part of the model development process. It helps to find the best model that represents our data. It also focuses on how well the chosen model will work in the future.\n",
    "\n",
    "We use the f-score to evaluate model performance. The F1-score or F1-measure is a measure of a model's accuracy in test data set. It is calculated from the precision and recall of the test data, where the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive.\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110e946",
   "metadata": {
    "id": "2110e946",
    "outputId": "7e47ae6d-6be9-48ec-d3a4-9b501b453f87"
   },
   "source": [
    "**Linear Support Vector Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fBQX39LCCENP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBQX39LCCENP",
    "outputId": "b8c7371a-a7f7-4d34-8e53-3ff1f2d33fe4"
   },
   "outputs": [],
   "source": [
    "print('f1- score:')\n",
    "\n",
    "print(metrics.f1_score(y_test, lsvc.predict(X_test), average = 'macro'))\n",
    "\n",
    "print('\\nConfusion Matrix\\n')\n",
    "\n",
    "print(confusion_matrix(y_test, lsvc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hGM3jniqIdUo",
   "metadata": {
    "id": "hGM3jniqIdUo"
   },
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88id95NOQ7sS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88id95NOQ7sS",
    "outputId": "5a11c4f0-adcf-4b41-e24e-2e4e05198693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1- score: 0.9986255849206738\n",
      "\n",
      "Confusion Matrix\n",
      "\n",
      "[[583   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 615   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 583   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 623   2   0   0   0   0   0   0]\n",
      " [  0   0   0   0 618   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 584   0   0   0   0   0]\n",
      " [  1   0   0   0   0   0 597   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 561   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 634   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 609   0]\n",
      " [  0   0   3   0   0   2   0   0   0   1 584]]\n"
     ]
    }
   ],
   "source": [
    "print('f1- score: '+ str(metrics.f1_score(y_test, nb.predict(X_test), average = 'macro')))\n",
    "\n",
    "print('\\nConfusion Matrix\\n')\n",
    "\n",
    "print(confusion_matrix(y_test, nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "RHEbXpAjPY_I",
   "metadata": {
    "id": "RHEbXpAjPY_I"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame( data = {'index': df_test['index'],\n",
    "                             'lang_id': nb.predict(df_test['clean_text']) })\n",
    "results.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154377c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Team 10 Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
